import torch
import torch.nn as nn
from torch.utils.data import Dataset, DataLoader
from transformers import BertTokenizer, BertForSequenceClassification





import json

with open('correctedIntents.json', 'r') as f:
  data = json.load(f)

# Create a dataset class to handle the data
class IntentDataset(Dataset):
  def __init__(self, encodings, labels):
    self.encodings = encodings
    self.labels = labels

  def __getitem__(self, idx):
    item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}
    item['labels'] = torch.tensor(self.labels[idx])
    return item

  def __len__(self):
    return len(self.labels)

# Tokenize the text data
tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')

# Prepare the input and output data for training
inputs = []
labels = []
for intent in data['intents']:
  for pattern in intent['patterns']:
    inputs.append(pattern)
    labels.append(intent['intent'])

# Tokenize the inputs
encodings = tokenizer(inputs, truncation=True, padding=True)

# Convert the labels to numerical indices
label_encoder = {label: i for i, label in enumerate(set(labels))}
encoded_labels = [label_encoder[label] for label in labels]

# Create the dataset and dataloaders
dataset = IntentDataset(encodings, encoded_labels)
train_loader = DataLoader(dataset, batch_size=16, shuffle=True)






# Use a pre-trained BERT model for sequence classification
model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=len(label_encoder))








# Define the loss function and optimizer
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model.to(device)
criterion = nn.CrossEntropyLoss()
optimizer = torch.optim.AdamW(model.parameters(), lr=5e-5)

# Train the model
for epoch in range(50):
  for batch in train_loader:
    optimizer.zero_grad()
    input_ids = batch['input_ids'].to(device)
    attention_mask = batch['attention_mask'].to(device)
    labels = batch['labels'].to(device)

    outputs = model(input_ids, attention_mask=attention_mask, labels=labels)
    loss = outputs.loss
    loss.backward()
    optimizer.step()

  print(f"Epoch {epoch+1}, Loss: {loss.item():.4f}")








model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=len(label_encoder))
model.load_state_dict(torch.load('Shanks.pth'))
model.to(device)







import random

# ... (rest of your code)

def chat():
    while True:
        user_input = input("You: ")
        if user_input == "quit":
            break

        inputs = tokenizer(user_input, return_tensors='pt').to(device)
        with torch.no_grad():
            outputs = model(**inputs)
            logits = outputs.logits
            predicted_class_id = torch.argmax(logits, dim=-1).item()

        # Find the corresponding intent in the data list
        for intent in data['intents']:
            if intent['intent'] == list(label_encoder.keys())[predicted_class_id]:
                response = random.choice(intent['responses'])
                break

        print(f"Bot: {response}")